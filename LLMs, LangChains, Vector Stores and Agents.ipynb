{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a4b8aa",
   "metadata": {},
   "source": [
    "# 0. Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34c0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69e299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fe1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7bf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56102bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52585a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8efd6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6630e3",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac24994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone as PineconeClient, ServerlessSpec\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_experimental.utilities.python import PythonREPL\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98024611",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776ace0",
   "metadata": {},
   "source": [
    "# 2. Using LangChain with Together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6695d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large language models are artificial intelligence models that have been trained on vast amounts of text data and can generate human-like text based on the input they receive.\n"
     ]
    }
   ],
   "source": [
    "# Load from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create the LangChain ChatOpenAI instance using Together's endpoint\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",  # You can use other Together-supported models too\n",
    "    openai_api_base=\"https://api.together.xyz/v1\",\n",
    "    openai_api_key=os.environ[\"TOGETHERAI_API_KEY\"], # Use the API key from the .env file\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Send a message\n",
    "response = llm.invoke([HumanMessage(content=\"Explain large language models in one sentence.\")])\n",
    "\n",
    "# Print the result\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e8358",
   "metadata": {},
   "source": [
    "# 3. Priming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1967872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help you with that! Here's an example Python script that trains a simple neural network using the Keras library on some simulated data. This script generates random input data and corresponding output labels, trains a neural network to map the inputs to the outputs, and then evaluates the network's performance.\n",
      "```python\n",
      "# Import necessary libraries\n",
      "import numpy as np\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "\n",
      "# Set random seed for reproducibility\n",
      "np.random.seed(42)\n",
      "\n",
      "# Generate some simulated input data and corresponding output labels\n",
      "# In this case, we'll generate 1000 data points with 10 input features each,\n",
      "# and a simple linear relationship between the inputs and outputs.\n",
      "# In practice, you would typically load your data from a file or database.\n",
      "num_data_points = 1000\n",
      "input_dim = 10\n",
      "output_dim = 1\n",
      "\n",
      "# Generate random input data\n",
      "X = np.random.rand(num_data_points, input_dim)\n",
      "\n",
      "# Generate corresponding output labels based on a simple linear relationship\n",
      "# with some added Gaussian noise\n",
      "w = np.random.rand(input_dim, output_dim)\n",
      "y = X.dot(w) + np.random.randn(num_data_points, output_dim)\n",
      "\n",
      "# Define and train the neural network\n",
      "model = Sequential()\n",
      "model.add(Dense(32, activation='relu', input_dim=input_dim))\n",
      "model.add(Dense(16, activation='relu'))\n",
      "model.add(Dense(output_dim))\n",
      "\n",
      "# Compile the model with mean squared error loss and Adam optimizer\n",
      "model.compile(loss='mse', optimizer='adam')\n",
      "\n",
      "# Train the model on the simulated data\n",
      "model.fit(X, y, epochs=100, batch_size=32)\n",
      "\n",
      "# Evaluate the model's performance on the training data\n",
      "training_loss = model.evaluate(X, y)\n",
      "print(\"Training loss: \", training_loss)\n",
      "\n",
      "# Generate some new input data to test the model's performance\n",
      "test_X = np.random.rand(100, input_dim)\n",
      "\n",
      "# Use the model to predict the corresponding output labels\n",
      "test_y_pred = model.predict(test_X)\n",
      "\n",
      "# Calculate the mean squared error between the predicted and true output labels\n",
      "test_loss = np.mean((test_X.dot(w) - test_y_pred) ** 2)\n",
      "print(\"Test loss: \", test_loss)\n",
      "```\n",
      "This script generates 1000 data points with 10 input features each, and trains a neural network with two hidden layers to map the inputs to the outputs. The network is trained using the mean squared error loss function and the Adam optimizer, and is evaluated on both the training data and some new test data. Note that this is just a simple example to illustrate the process of training a neural network on simulated data, and in practice you would typically use more sophisticated data generation and network architecture techniques.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    openai_api_base=\"https://api.together.xyz/v1\",\n",
    "    openai_api_key=os.environ[\"TOGETHERAI_API_KEY\"],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "print(response.content, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d419a00",
   "metadata": {},
   "source": [
    "# 4. Creating and using prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec63f525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], template='\\nYou are an expert data scientist with an expertise in building deep learning models.\\nExplain the concept of {concept} in a couple of lines.\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models.\n",
    "Explain the concept of {concept} in a couple of lines.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c8f046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to explain the concept of regularization in a few lines!\n",
      "\n",
      "Regularization is a technique used in machine learning, including deep learning, to prevent overfitting of models to training data. It works by adding a penalty term to the loss function, which discourages the model from learning overly complex relationships between the features and the target variable. This penalty term typically takes the form of the L1 or L2 norm of the model's weights, which encourages the weights to be small and sparse, thereby reducing the model's capacity and preventing overfitting. Regularization can help improve the generalization performance of the model and lead to better performance on unseen data.\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt and wrap in HumanMessage\n",
    "formatted_prompt = prompt.format(concept=\"regularization\")\n",
    "\n",
    "response = llm.invoke([HumanMessage(content=formatted_prompt)])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89339340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-26e565370c95>:2: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = llm([HumanMessage(content=formatted_prompt)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " An autoencoder is a type of artificial neural network used for learning efficient codings of input data. It consists of two parts: an encoder, which maps the input data to a lower-dimensional representation, and a decoder, which maps this lower-dimensional representation back to the original data space. The goal of an autoencoder is to minimize the difference between the input and the reconstructed output, thereby learning a compact and informative representation of the input data. Autoencoders have many applications, including dimensionality reduction, anomaly detection, and generative modeling.\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt.format(concept=\"autoencoder\")\n",
    "response = llm([HumanMessage(content=formatted_prompt)])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0717df",
   "metadata": {},
   "source": [
    "# 5. Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a6c3490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " An autoencoder is a type of artificial neural network used for learning efficient codings of input data. It's comprised of two parts: an encoder, which maps the input to a lower-dimensional representation, and a decoder, which maps this representation back to the original dimension. The goal of an autoencoder is to minimize the difference between the input and the output, thereby learning a compact and informative representation of the input data. Autoencoders have found applications in various domains, including dimensionality reduction, anomaly detection, and generative modeling.\n"
     ]
    }
   ],
   "source": [
    "# Create a chain using the new RunnableSequence method\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\"concept\": \"autoencoder\"})\n",
    "\n",
    "# Print the result\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7626d64e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to explain autoencoders in a simple way!\n",
      "\n",
      "Imagine you have a big box of lego blocks of different colors and shapes. Now, your task is to build a small, exact replica of this box using only some of the lego blocks. You can only look at the big box and try to memorize the contents, but you can't open it or take out the blocks. Then, you go to a separate table and try to build the replica using the limited number of blocks you have. This is essentially what an autoencoder does!\n",
      "\n",
      "In data science terms, an autoencoder is a type of neural network that is trained to copy its input to its output, but with a twist. The network has a narrow \"bottleneck\" in the middle that reduces the number of features or dimensions of the input data. This forces the network to learn a compact and efficient representation of the input data, just like how you had to build a small replica of the big box using fewer lego blocks.\n",
      "\n",
      "The autoencoder consists of two parts: the encoder and the decoder. The encoder takes the input data and maps it to a lower-dimensional representation, called the bottleneck or latent representation. The decoder then takes this compact representation and maps it back to the original input space, trying to reconstruct the original input as closely as possible.\n",
      "\n",
      "During training, the autoencoder learns to minimize the difference between the input and the reconstructed output, typically using a loss function such as mean squared error. This encourages the network to learn a more efficient and informative representation of the input data in the bottleneck.\n",
      "\n",
      "Once the autoencoder is trained, we can use the encoder to extract features or representations from new, unseen data. This is useful for tasks such as anomaly detection, where we want to identify data points that are significantly different from the norm. By comparing the latent representation of a new data point to a reference distribution of latent representations learned from the training data, we can detect anomalies that stand out from the crowd.\n",
      "\n",
      "In summary, an autoencoder is a neural network that learns to compress and reconstruct its input data in a lower-dimensional space, forcing it to learn a more efficient and informative representation of the data. This can be used for various applications such as anomaly detection, dimensionality reduction, and generative modeling. Just like how you built a small replica of the big box using fewer lego blocks, an autoencoder learns to represent complex data using a more compact and informative representation.\n"
     ]
    }
   ],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\"\n",
    ")\n",
    "\n",
    "chain_two = prompt | second_prompt | llm\n",
    "\n",
    "response = chain_two.invoke({\"concept\": \"autoencoder\"})\n",
    "explanation = response.content\n",
    "\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bfad8",
   "metadata": {},
   "source": [
    "# 6. Document chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ccf144b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Sure, I'd be happy to explain autoencoders in a simple way!\"),\n",
       " Document(page_content='Imagine you have a big box of lego blocks of different colors and shapes. Now, your task is to'),\n",
       " Document(page_content='build a small, exact replica of this box using only some of the lego blocks. You can only look at'),\n",
       " Document(page_content=\"the big box and try to memorize the contents, but you can't open it or take out the blocks. Then,\"),\n",
       " Document(page_content='you go to a separate table and try to build the replica using the limited number of blocks you'),\n",
       " Document(page_content='have. This is essentially what an autoencoder does!'),\n",
       " Document(page_content='In data science terms, an autoencoder is a type of neural network that is trained to copy its input'),\n",
       " Document(page_content='to its output, but with a twist. The network has a narrow \"bottleneck\" in the middle that reduces'),\n",
       " Document(page_content='the number of features or dimensions of the input data. This forces the network to learn a compact'),\n",
       " Document(page_content='and efficient representation of the input data, just like how you had to build a small replica of'),\n",
       " Document(page_content='the big box using fewer lego blocks.'),\n",
       " Document(page_content='The autoencoder consists of two parts: the encoder and the decoder. The encoder takes the input'),\n",
       " Document(page_content='data and maps it to a lower-dimensional representation, called the bottleneck or latent'),\n",
       " Document(page_content='representation. The decoder then takes this compact representation and maps it back to the original'),\n",
       " Document(page_content='input space, trying to reconstruct the original input as closely as possible.'),\n",
       " Document(page_content='During training, the autoencoder learns to minimize the difference between the input and the'),\n",
       " Document(page_content='reconstructed output, typically using a loss function such as mean squared error. This encourages'),\n",
       " Document(page_content='the network to learn a more efficient and informative representation of the input data in the'),\n",
       " Document(page_content='bottleneck.'),\n",
       " Document(page_content='Once the autoencoder is trained, we can use the encoder to extract features or representations from'),\n",
       " Document(page_content='new, unseen data. This is useful for tasks such as anomaly detection, where we want to identify'),\n",
       " Document(page_content='data points that are significantly different from the norm. By comparing the latent representation'),\n",
       " Document(page_content='of a new data point to a reference distribution of latent representations learned from the training'),\n",
       " Document(page_content='data, we can detect anomalies that stand out from the crowd.'),\n",
       " Document(page_content='In summary, an autoencoder is a neural network that learns to compress and reconstruct its input'),\n",
       " Document(page_content='data in a lower-dimensional space, forcing it to learn a more efficient and informative'),\n",
       " Document(page_content='representation of the data. This can be used for various applications such as anomaly detection,'),\n",
       " Document(page_content='dimensionality reduction, and generative modeling. Just like how you built a small replica of the'),\n",
       " Document(page_content='big box using fewer lego blocks, an autoencoder learns to represent complex data using a more'),\n",
       " Document(page_content='compact and informative representation.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation])\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43df60ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, I'd be happy to explain autoencoders in a simple way!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae3828e",
   "metadata": {},
   "source": [
    "# 7. Creating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "616e3cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.1283787339925766,\n",
       " -0.060270845890045166,\n",
       " -0.044977813959121704,\n",
       " 0.007588156033307314,\n",
       " -0.035884466022253036,\n",
       " 0.024218657985329628,\n",
       " -0.049276966601610184,\n",
       " -0.043137647211551666,\n",
       " -0.05208056420087814,\n",
       " -0.013494600541889668,\n",
       " 0.039489977061748505,\n",
       " 0.01161170657724142,\n",
       " -0.002269306220114231,\n",
       " 0.028034480288624763,\n",
       " -0.06034579873085022,\n",
       " 0.028864778578281403,\n",
       " -0.06871556490659714,\n",
       " 0.07362587004899979,\n",
       " -0.12378135323524475,\n",
       " -0.0477871298789978,\n",
       " 0.03414962440729141,\n",
       " -0.02835344523191452,\n",
       " -0.0286081675440073,\n",
       " 0.0028957908507436514,\n",
       " 0.031387828290462494,\n",
       " 0.012274770997464657,\n",
       " 0.011112072505056858,\n",
       " 0.04263579845428467,\n",
       " 0.07507694512605667,\n",
       " -0.03126320615410805,\n",
       " 0.030870916321873665,\n",
       " 0.024923639371991158,\n",
       " 0.07557026296854019,\n",
       " 0.10439079999923706,\n",
       " -0.044207628816366196,\n",
       " 0.031903624534606934,\n",
       " -0.05027369037270546,\n",
       " 0.056193914264440536,\n",
       " -0.045568469911813736,\n",
       " -0.03324221819639206,\n",
       " -0.012522927485406399,\n",
       " -0.01810121163725853,\n",
       " -0.00423078378662467,\n",
       " 0.007492333650588989,\n",
       " 0.014306861907243729,\n",
       " 0.006017929874360561,\n",
       " -0.028029857203364372,\n",
       " -0.014105167239904404,\n",
       " -0.0921512097120285,\n",
       " -0.04260644689202309,\n",
       " -0.03884457051753998,\n",
       " 0.00949779897928238,\n",
       " -0.02597631886601448,\n",
       " -0.026680607348680496,\n",
       " 0.0037024517077952623,\n",
       " -0.004862628877162933,\n",
       " 0.0135462312027812,\n",
       " -0.04004642739892006,\n",
       " -0.06135152280330658,\n",
       " -0.017850738018751144,\n",
       " 0.015621164813637733,\n",
       " 0.023756032809615135,\n",
       " -0.04311145469546318,\n",
       " 0.06071273982524872,\n",
       " 0.07986897975206375,\n",
       " 0.014620772562921047,\n",
       " 0.008128752931952477,\n",
       " 0.06370193511247635,\n",
       " -0.0034704592544585466,\n",
       " 0.004041051492094994,\n",
       " -0.015867076814174652,\n",
       " -0.02471357211470604,\n",
       " -0.012658960185945034,\n",
       " 0.01589089259505272,\n",
       " 0.07465413212776184,\n",
       " -0.015378735028207302,\n",
       " -0.06633544713258743,\n",
       " -0.0014120055129751563,\n",
       " 0.009734268300235271,\n",
       " 0.00705276895314455,\n",
       " -0.0262109637260437,\n",
       " 0.0106964111328125,\n",
       " 0.04389672726392746,\n",
       " 0.056004904210567474,\n",
       " 0.01872963085770607,\n",
       " 0.03648897260427475,\n",
       " 0.011556800454854965,\n",
       " 0.020342253148555756,\n",
       " 0.0011177163105458021,\n",
       " 0.03556227684020996,\n",
       " -0.08050055801868439,\n",
       " -0.04669457674026489,\n",
       " 0.027391908690333366,\n",
       " 0.0032550834584981203,\n",
       " -0.01693268120288849,\n",
       " -0.020908135920763016,\n",
       " 0.07534857094287872,\n",
       " -0.014934764243662357,\n",
       " -0.03540222719311714,\n",
       " 0.004473820794373751,\n",
       " -2.482534728187602e-05,\n",
       " -0.02355450950562954,\n",
       " -0.02900814451277256,\n",
       " -0.040736325085163116,\n",
       " -0.02806694805622101,\n",
       " -0.002604082692414522,\n",
       " 0.05428342893719673,\n",
       " 0.08087966591119766,\n",
       " 0.025235094130039215,\n",
       " -0.08743828535079956,\n",
       " -0.04455667734146118,\n",
       " 0.0262751504778862,\n",
       " 0.04087722301483154,\n",
       " -0.06466463953256607,\n",
       " 0.007962497882544994,\n",
       " 0.009796440601348877,\n",
       " -0.02868795022368431,\n",
       " 0.019603470340371132,\n",
       " 0.12559457123279572,\n",
       " -0.0033138529397547245,\n",
       " -0.01786348782479763,\n",
       " -0.028773872181773186,\n",
       " -0.08435624092817307,\n",
       " 0.05388287454843521,\n",
       " 0.06347301602363586,\n",
       " -0.07969555258750916,\n",
       " -0.00373248104006052,\n",
       " -3.298003684865473e-33,\n",
       " 0.0020830375142395496,\n",
       " 0.0326850451529026,\n",
       " -0.03400813788175583,\n",
       " 0.01599113643169403,\n",
       " -0.016934923827648163,\n",
       " -0.032739460468292236,\n",
       " -0.09640666097402573,\n",
       " -0.015367318876087666,\n",
       " 0.005984855815768242,\n",
       " 0.01696479134261608,\n",
       " -0.027921048924326897,\n",
       " 0.020247502252459526,\n",
       " -0.010587392374873161,\n",
       " 0.16584233939647675,\n",
       " -0.015654796734452248,\n",
       " 0.011966093443334103,\n",
       " -0.04234861955046654,\n",
       " 0.014137325808405876,\n",
       " 0.0909518301486969,\n",
       " -0.06656776368618011,\n",
       " 0.026892460882663727,\n",
       " 0.023201867938041687,\n",
       " -0.06464114040136337,\n",
       " -0.031286802142858505,\n",
       " -0.024145180359482765,\n",
       " 0.07757467031478882,\n",
       " 0.05497533455491066,\n",
       " -0.03299116715788841,\n",
       " 0.06817807257175446,\n",
       " 0.005976694170385599,\n",
       " -0.0916464701294899,\n",
       " -0.02311834879219532,\n",
       " -0.03561298921704292,\n",
       " -0.0068358504213392735,\n",
       " -0.01065795961767435,\n",
       " 0.027570784091949463,\n",
       " 0.020887665450572968,\n",
       " 0.06146596744656563,\n",
       " -0.030324893072247505,\n",
       " -0.033340565860271454,\n",
       " 0.07318061590194702,\n",
       " -0.0033675790764391422,\n",
       " -0.03009706176817417,\n",
       " -0.034548889845609665,\n",
       " -0.013944762758910656,\n",
       " -0.0375126488506794,\n",
       " 0.006573354359716177,\n",
       " -0.0034945232328027487,\n",
       " -0.07755205035209656,\n",
       " 0.003964442294090986,\n",
       " 0.02130703628063202,\n",
       " 0.04439028725028038,\n",
       " -0.022981707006692886,\n",
       " -0.05077749118208885,\n",
       " 0.05446917191147804,\n",
       " 0.04542014002799988,\n",
       " 7.941560033941641e-05,\n",
       " 0.04132372513413429,\n",
       " 0.04968668520450592,\n",
       " 0.04467292129993439,\n",
       " -0.007134379353374243,\n",
       " 0.06512933224439621,\n",
       " 0.08775996416807175,\n",
       " -0.018948927521705627,\n",
       " 0.018201196566224098,\n",
       " -0.013194278813898563,\n",
       " 0.004195726476609707,\n",
       " -0.061290282756090164,\n",
       " 0.06658878922462463,\n",
       " 0.06578318029642105,\n",
       " -0.04839878901839256,\n",
       " 0.05826180428266525,\n",
       " 0.0026271375827491283,\n",
       " -0.018608463928103447,\n",
       " 0.0566311776638031,\n",
       " 0.04853183031082153,\n",
       " -0.04893520846962929,\n",
       " -0.06847485899925232,\n",
       " 0.01477036066353321,\n",
       " 0.04484553635120392,\n",
       " -0.02942713536322117,\n",
       " 0.018860524520277977,\n",
       " -0.02592485584318638,\n",
       " 0.08309707045555115,\n",
       " -0.035155288875103,\n",
       " -0.0018977542640641332,\n",
       " 0.05499057099223137,\n",
       " -0.09591855108737946,\n",
       " -0.043163053691387177,\n",
       " 0.0028404854238033295,\n",
       " 0.055982623249292374,\n",
       " -0.053117670118808746,\n",
       " 0.016884222626686096,\n",
       " -0.04468671232461929,\n",
       " -0.005871233064681292,\n",
       " 1.953600163050631e-33,\n",
       " -0.014467376284301281,\n",
       " 0.05145116150379181,\n",
       " -0.07325092703104019,\n",
       " 0.08989068865776062,\n",
       " -0.0522766187787056,\n",
       " 5.881195102119818e-05,\n",
       " -0.003704202827066183,\n",
       " 0.08429396152496338,\n",
       " -0.000229402823606506,\n",
       " 0.006793102715164423,\n",
       " -0.013046732172369957,\n",
       " -0.01274521742016077,\n",
       " -0.05362781509757042,\n",
       " -0.01140669733285904,\n",
       " -0.02943657897412777,\n",
       " -0.01763814501464367,\n",
       " -0.04993503540754318,\n",
       " 0.03590519726276398,\n",
       " 0.034090299159288406,\n",
       " 0.06319598853588104,\n",
       " -0.017036762088537216,\n",
       " 0.11425217241048813,\n",
       " -0.06139775738120079,\n",
       " -0.01943347416818142,\n",
       " 0.03372164070606232,\n",
       " 0.05117795616388321,\n",
       " 0.005165063310414553,\n",
       " 0.135799840092659,\n",
       " 0.03163757175207138,\n",
       " 0.021196510642766953,\n",
       " -0.015663284808397293,\n",
       " -0.046487998217344284,\n",
       " -0.039137501269578934,\n",
       " -0.030883369967341423,\n",
       " -0.05661623924970627,\n",
       " 0.018217559903860092,\n",
       " 0.032265555113554,\n",
       " -0.012450315058231354,\n",
       " -0.058566417545080185,\n",
       " 0.030477771535515785,\n",
       " 0.06625112891197205,\n",
       " -0.05095771700143814,\n",
       " -0.052417486906051636,\n",
       " 0.011797444894909859,\n",
       " -0.021473513916134834,\n",
       " -0.019337618723511696,\n",
       " 0.02664056420326233,\n",
       " 0.039596304297447205,\n",
       " 0.019347120076417923,\n",
       " 0.00020980657427571714,\n",
       " -0.015373497270047665,\n",
       " 0.010743897408246994,\n",
       " 0.010487299412488937,\n",
       " -0.06664036959409714,\n",
       " -0.018772495910525322,\n",
       " -0.01843714714050293,\n",
       " 0.0959453284740448,\n",
       " -0.023525657132267952,\n",
       " 0.04983409494161606,\n",
       " 0.0542784221470356,\n",
       " -0.08745834976434708,\n",
       " -0.13049499690532684,\n",
       " 0.11556290090084076,\n",
       " -0.0565502904355526,\n",
       " -0.01820988394320011,\n",
       " -0.09009450674057007,\n",
       " -0.07845968008041382,\n",
       " 0.032264117151498795,\n",
       " -0.0006082820473238826,\n",
       " -0.03332367539405823,\n",
       " 0.18405012786388397,\n",
       " -0.011950611136853695,\n",
       " -0.023643426597118378,\n",
       " 0.102645143866539,\n",
       " 0.08856914192438126,\n",
       " -0.11358696967363358,\n",
       " 0.049281880259513855,\n",
       " -0.05244357883930206,\n",
       " -0.018082980066537857,\n",
       " -0.10240602493286133,\n",
       " -0.02659786492586136,\n",
       " -0.01768781989812851,\n",
       " 0.08131386339664459,\n",
       " 0.04861902445554733,\n",
       " -0.042602330446243286,\n",
       " 0.05188656225800514,\n",
       " -0.025323057547211647,\n",
       " 0.003129643388092518,\n",
       " 0.03742922469973564,\n",
       " 0.013973399996757507,\n",
       " -0.024314237758517265,\n",
       " 0.14064303040504456,\n",
       " 0.07636857032775879,\n",
       " 0.03754645213484764,\n",
       " -0.07745474576950073,\n",
       " -2.2182359060707313e-08,\n",
       " -0.038442838937044144,\n",
       " -0.023714549839496613,\n",
       " 0.08940098434686661,\n",
       " -0.0014947260497137904,\n",
       " -0.031030112877488136,\n",
       " -0.05933346599340439,\n",
       " 0.008450891822576523,\n",
       " 0.03875532001256943,\n",
       " -0.06782066076993942,\n",
       " -0.03826682269573212,\n",
       " 0.08257252722978592,\n",
       " -0.03000630810856819,\n",
       " -0.11704760789871216,\n",
       " 0.029362211003899574,\n",
       " -0.004199143964797258,\n",
       " 0.10933731496334076,\n",
       " 0.04959851875901222,\n",
       " 0.024283181875944138,\n",
       " -0.02780155837535858,\n",
       " 0.03461899980902672,\n",
       " -0.009400037117302418,\n",
       " 0.13057199120521545,\n",
       " -0.05847713351249695,\n",
       " -0.011840242892503738,\n",
       " 0.013269509188830853,\n",
       " -0.029999002814292908,\n",
       " -0.01565397158265114,\n",
       " 0.07735984772443771,\n",
       " 0.01836223714053631,\n",
       " -0.020289761945605278,\n",
       " 0.027106953784823418,\n",
       " 0.0825563371181488,\n",
       " 0.03946458548307419,\n",
       " 0.009878543205559254,\n",
       " -0.02614276483654976,\n",
       " 0.009927000850439072,\n",
       " -0.02511606365442276,\n",
       " -0.11402781307697296,\n",
       " -0.05153662711381912,\n",
       " -0.0823298916220665,\n",
       " 0.025490999221801758,\n",
       " -0.014710760675370693,\n",
       " -0.11704458296298981,\n",
       " 0.007158270105719566,\n",
       " 0.024753928184509277,\n",
       " -0.0024682225193828344,\n",
       " 0.09777359664440155,\n",
       " -0.15930716693401337,\n",
       " -0.04343589022755623,\n",
       " -0.027444852516055107,\n",
       " 0.04408443719148636,\n",
       " 0.0012012387160211802,\n",
       " 0.030582817271351814,\n",
       " -0.0053421384654939175,\n",
       " 0.05408784747123718,\n",
       " -0.03791091963648796,\n",
       " 0.04728430137038231,\n",
       " -0.012102554552257061,\n",
       " 0.041451115161180496,\n",
       " 0.08144714683294296,\n",
       " -0.052351221442222595,\n",
       " 0.15090137720108032,\n",
       " 0.010674063116312027,\n",
       " -0.004461485426872969]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a45e81",
   "metadata": {},
   "source": [
    "# 8. Storing embeddings in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "475aa92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='idea is that the autoencoder learns to keep only the most important features of the input data in'),\n",
       " Document(page_content='An autoencoder is a type of machine learning model that can learn to compress and then reconstruct'),\n",
       " Document(page_content='One cool thing about autoencoders is that they can be used for things like image compression,'),\n",
       " Document(page_content='One cool thing about autoencoders is that they can be used for things like image compression,')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Pinecone SDK client\n",
    "pc = PineconeClient(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    ")\n",
    "\n",
    "# Create or use an index\n",
    "index_name = \"langchain-quickstart\"\n",
    "\n",
    "# Only if index doesn't already exist\n",
    "# pc.create_index(\n",
    "#     index_name,\n",
    "#     dimension=384,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "# )\n",
    "\n",
    "# Now connect LangChain to Pinecone via vector store\n",
    "vectorstore = Pinecone.from_documents(\n",
    "    documents=texts,               # your LangChain Document[] list\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# Do a search\n",
    "query = \"What is magical about an autoencoder?\"\n",
    "\n",
    "result = vectorstore.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053cd9b",
   "metadata": {},
   "source": [
    "# 9. Creating and using an agent for reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deca8ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m To find the roots (zeroes) of the quadratic function3 * x**2 +2*x -1, we can use the quadratic formula:x = [-b ± sqrt(b**2 - 4ac)] / 2a\n",
      "\n",
      "where a = 3, b = 2, and c = -1.\n",
      "\n",
      "Substituting these values into the formula, we get:\n",
      "\n",
      "x = [-2 ± sqrt(2**2 - 4(3)(-1))] / 2(3)\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "x = [-2 ± sqrt(4 + 12)] / 6\n",
      "\n",
      "x = [-2 ± sqrt(16)] / 6\n",
      "\n",
      "x = [-2 ± 4] / 6\n",
      "\n",
      "So the two roots (zeroes) of the quadratic function are:\n",
      "\n",
      "x = (-2 + 4) / 6 = 0\n",
      "\n",
      "and\n",
      "\n",
      "x = (-2 - 4) / 6 = -2/3\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Find the roots (zeroes) of the quadratic function 3 * x**2 + 2*x - 1', 'output': ' To find the roots (zeroes) of the quadratic function3 * x**2 +2*x -1, we can use the quadratic formula:x = [-b ± sqrt(b**2 - 4ac)] / 2a\\n\\nwhere a = 3, b = 2, and c = -1.\\n\\nSubstituting these values into the formula, we get:\\n\\nx = [-2 ± sqrt(2**2 - 4(3)(-1))] / 2(3)\\n\\nSimplifying, we get:\\n\\nx = [-2 ± sqrt(4 + 12)] / 6\\n\\nx = [-2 ± sqrt(16)] / 6\\n\\nx = [-2 ± 4] / 6\\n\\nSo the two roots (zeroes) of the quadratic function are:\\n\\nx = (-2 + 4) / 6 = 0\\n\\nand\\n\\nx = (-2 - 4) / 6 = -2/3'}\n"
     ]
    }
   ],
   "source": [
    "# Together.ai LLM setup\n",
    "llm = ChatOpenAI(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    openai_api_key=os.getenv(\"TOGETHERAI_API_KEY\"),\n",
    "    openai_api_base=\"https://api.together.xyz/v1\",\n",
    "    temperature=0,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# Tool setup\n",
    "tools = [PythonREPLTool()]\n",
    "\n",
    "# Prompt setup\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant that uses Python for computation.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Create agent and executor\n",
    "agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run task\n",
    "result = agent_executor.invoke({\n",
    "    \"input\": \"Find the roots (zeroes) of the quadratic function 3 * x**2 + 2*x - 1\"\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a3b5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6f9ee",
   "metadata": {},
   "source": [
    "# 10. Calculating code execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc7a8659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 66 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total runtime: {round(end - start)} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
